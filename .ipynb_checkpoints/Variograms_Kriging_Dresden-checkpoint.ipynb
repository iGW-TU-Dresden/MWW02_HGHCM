{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db261183",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a839b6",
   "metadata": {},
   "source": [
    "## Variograms Example - Ground Elevation Data for Dresden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414d668f",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b6cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import least_squares\n",
    "from ipywidgets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf8597a",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<em> <font size=4> \"Everything is related to everything else, but near things are more related than distant things!\"  </font size></em> \n",
    "<br>\n",
    "<i> Waldo Tobler (1970) </i> </div align>\n",
    "\n",
    "### Variograms\n",
    "In order to evaluate the dependency of a variable's value on its position, all samples are compared with regard to their similarity and proximity. We will use a small data set with the ground height at 35 randomly determined positions in the city of Dresden:\n",
    "### Load Data from `.csv`-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a808e64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ort</th>\n",
       "      <th>Nummer</th>\n",
       "      <th>Rechtswert [m]</th>\n",
       "      <th>Hochwert [m]</th>\n",
       "      <th>Hoehe [m ue. NN]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Altmarkt</td>\n",
       "      <td>1</td>\n",
       "      <td>5411525</td>\n",
       "      <td>5656080</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zwinger</td>\n",
       "      <td>2</td>\n",
       "      <td>5411250</td>\n",
       "      <td>5656485</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hbf</td>\n",
       "      <td>3</td>\n",
       "      <td>5411185</td>\n",
       "      <td>5655012</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chemiebau</td>\n",
       "      <td>4</td>\n",
       "      <td>5410975</td>\n",
       "      <td>5653670</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wasaplatz</td>\n",
       "      <td>5</td>\n",
       "      <td>5413006</td>\n",
       "      <td>5653677</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Ort  Nummer  Rechtswert [m]  Hochwert [m]  Hoehe [m ue. NN]\n",
       "0   Altmarkt       1         5411525       5656080               114\n",
       "1    Zwinger       2         5411250       5656485               110\n",
       "2        Hbf       3         5411185       5655012               115\n",
       "3  Chemiebau       4         5410975       5653670               143\n",
       "4  Wasaplatz       5         5413006       5653677               123"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data from the excel-file, resulting in a pandas.DataFrame\n",
    "#     (a table-like data structure)\n",
    "# full path, e.g., \"C:/User/.../HGHCM/Data_Dresden.csv\" can be used as well\n",
    "df = pd.read_csv(\"Data_Dresden.csv\", sep=\";\", encoding='latin-1')\n",
    "\n",
    "# print shape of data (n_rows, n_cols)\n",
    "print(df.shape)\n",
    "# print first 5 rows of data set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ab07e",
   "metadata": {},
   "source": [
    "### Cumulative Histogram\n",
    "In order to get an overview on the distribution of ground heights all over the city, we will take a look on a cumulative histogram of the values, assuming that the sum curve of the real ground heights would look similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "558d53ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7c8664eb4d469ead10a62d5b2412d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate mean and median\n",
    "# with DataFrame.iloc, we can access the values in the table indexed by\n",
    "#     [row, col] --> (a colon \":\" can be used to say something like \"from here\n",
    "#     until the end\" or \"from the beginning until here\")\n",
    "mean = np.mean(df.iloc[:,-1])\n",
    "median = np.median(df.iloc[:, -1])\n",
    "\n",
    "# plot histogram\n",
    "ax = df.iloc[:, -1].hist(cumulative=True, density=True, histtype=\"step\", lw=2,\n",
    "                         bins=df.shape[0], label=\"Ground Height\", figsize=(10, 6))\n",
    "# plot mean and median\n",
    "ax.axvline(x=median, ls='--', color='red', label=\"Median\")\n",
    "ax.axvline(x=mean, ls='--', color='green', label=\"Mean\")\n",
    "\n",
    "# set axis labels, make settings\n",
    "ax.set_xlabel(\"ground height [MASL]\")\n",
    "ax.set_ylabel(\"density\")\n",
    "ax.set_title(\"Histogram of Ground Height\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlim(100, 300)\n",
    "ax.grid(True, alpha=0.3)\n",
    "l = ax.legend(loc=(0.7, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f66a29",
   "metadata": {},
   "source": [
    "The green line gives the arithmetic mean of all measured ground heights, the red line gives the median. As the median crosses the sum curve at approximately <span>&asymp;</span>140 m, whilst the arithmetic mean of the values is much bigger (<span>&asymp;</span>165 m), we can assume that the ground height is <b>not</b> normally distributed among the area. This indicates, that most of the <i>conventional</i> statistical measures (such as variance, standard deviation, arithmetic mean) are not meaningful, and a more sophisticated statistical evaluation is needed. However, as we are using the geographical position as additional information, the lack of a normal distribution is <b>not</b> problematic for us any more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ea0c2",
   "metadata": {},
   "source": [
    "### Calculate Distances\n",
    "In order to assess the dependency of the ground height from its position (and thus from its proximity of one known point to another), we have to create an empirical variogram. For this, we need to forget about the absolute values and positions of our \"sampling points\" and calculate the distances in space and the height differences between all possible pairs of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd38506",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize lists to store distance values in\n",
    "distances = []\n",
    "hdiffs = []\n",
    "\n",
    "# iterate through all points (outter loop)\n",
    "# the indexing of the table is as before\n",
    "# with zip(...), we can combine multiple lists; when we then iterate over the zip,\n",
    "#     we get a tuple (value0 from list0, value1 from list1, ...) at each iteration\n",
    "for point1 in zip(df.iloc[:, 2], df.iloc[:, 3], df.iloc[:, 4]):\n",
    "    # make lists to store information in relation to each point in the outter\n",
    "    #     iteration loop\n",
    "    dist_from_point1 = []\n",
    "    hdiff_from_point1 = []\n",
    "    # iterate through all points (inner loop)\n",
    "    for point2 in zip(df.iloc[:, 2], df.iloc[:, 3], df.iloc[:, 4]):\n",
    "        \n",
    "        # calculate the distance in the x-direction dx\n",
    "        dx = point2[0] - point1[0]\n",
    "        \n",
    "        # calculate the distance in the y-direction dy\n",
    "        dy = point2[1] - point1[1]\n",
    "        \n",
    "        # calculate abolute distance\n",
    "        dist = ((dx ** 2) + (dy ** 2)) ** 0.5\n",
    "        \n",
    "        # calculate SQUARED height difference\n",
    "        hdiff = (point1[2] - point2[2]) ** 2\n",
    "        \n",
    "        # append results to dist_from_point1 and hdiff_from_point1\n",
    "        dist_from_point1.append(dist)\n",
    "        hdiff_from_point1.append(hdiff)\n",
    "        \n",
    "    \n",
    "    # append all distances from one point to all\n",
    "    # other points to the distances array\n",
    "    distances.append(dist_from_point1)\n",
    "    hdiffs.append(hdiff_from_point1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e7880c",
   "metadata": {},
   "source": [
    "### Visual Representation of Distances and Ground Height Differences\n",
    "We can get a overview on the distances between the different sampling points by creating a color bar. The same can be done for the (squared) height differences between the points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34079a93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5d0da75aab4bc8bcfc9cd86a551704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up a figure with two subplots (one row, two columns)\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "\n",
    "# plot representation of distances\n",
    "im1 = ax[0].imshow(distances, cmap=\"rainbow\")\n",
    "# make colorbar\n",
    "cbar1 = plt.colorbar(im1, shrink=0.3, ax=ax[0])\n",
    "# labels etc.\n",
    "cbar1.set_label(\"distance [m]\")\n",
    "ax[0].set_title(\"Distances Between Points\")\n",
    "ax[0].set_xlabel(\"point index\")\n",
    "ax[0].set_ylabel(\"point_index\")\n",
    "\n",
    "# plot represenation of squared height differences\n",
    "im2 = ax[1].imshow(hdiffs, cmap=\"rainbow\")\n",
    "# make colorbar\n",
    "cbar2 = plt.colorbar(im2, shrink=0.3, ax=ax[1])\n",
    "# labels etc.\n",
    "cbar2.set_label(\"squared height difference [m]\")\n",
    "ax[1].set_title(\"Squared Height Difference Between Points\")\n",
    "ax[1].set_xlabel(\"point index\")\n",
    "ax[1].set_ylabel(\"point_index\")\n",
    "\n",
    "# use tight layout to correct for overlapping parts of figure\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b850da2",
   "metadata": {},
   "source": [
    "### Sort Distances\n",
    "In order to create an empirical variogram, we want to rank all pairs of values (i.e., distances and height differences between points), starting from the pairs with the smallest distance between each other. Ultimately, we want to have a data-structure (or table) with the rows being individual pairs of points and the columns being something like \"point 1\", \"point 2\", \"distance\", and \"height difference\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e5aee3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize list to store distances and other information\n",
    "preprocessed_data = []\n",
    "\n",
    "# iterate over upper part of data matrices, excluding the main diagonal\n",
    "#     because matrix is square, symmetric and main diagonal only contains\n",
    "#     zeros in both cases\n",
    "for row in range(0, len(distances) - 1):\n",
    "    for col in range(row + 1, len(distances)):\n",
    "        \n",
    "        # information = [point1, point2, dist, hdiff]\n",
    "        # access distances and hdiffs by row and column indices\n",
    "        information = [row, col, distances[row][col], hdiffs[row][col]]\n",
    "        \n",
    "        # save data\n",
    "        preprocessed_data.append(information)\n",
    "        \n",
    "# put data to DataFrame\n",
    "df_preprocessed = pd.DataFrame(data=preprocessed_data, columns=[\"point1\", \"point2\",\n",
    "                                                                \"distance\", \"hdiff\"])\n",
    "\n",
    "# sort data according to distance\n",
    "df_sorted = df_preprocessed.sort_values(by=[\"distance\"])\n",
    "# reindex DataFrame to start at 0 again and increase because the sorting also\n",
    "#     influences the indices of the rows\n",
    "df_sorted.index = [i for i in range(len(df_sorted))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00e8a531",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point1</th>\n",
       "      <th>point2</th>\n",
       "      <th>distance</th>\n",
       "      <th>hdiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>489.540601</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>993.227064</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>33</td>\n",
       "      <td>1090.875337</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1120.813990</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1182.642803</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   point1  point2     distance  hdiff\n",
       "0       0       1   489.540601     16\n",
       "1       2      29   993.227064      1\n",
       "2      25      33  1090.875337      1\n",
       "3       0       2  1120.813990      1\n",
       "4       0      29  1182.642803      4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2947648",
   "metadata": {},
   "source": [
    "### Function to Calculate Empirical Variogram Values Based on Number of Classes / Number of Values in each Class\n",
    "The sorted values are summarized in classes and the average distance between all values in one class is defined as the \"class center\". Averaging the height difference in this class allows to plot each class in the empirical variogram: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5514ed5f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def variogram(data, class_col, var_col, nvals, plotting=True):\n",
    "    \"\"\"\n",
    "    data : pd.DataFrame containing relevant and sorted data, pd.DataFrame\n",
    "    class_col: column to use to calculate class centers, int\n",
    "    var_col : column to use for the variogram data, int\n",
    "    nvals : number of values to consider in each class, int\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the number of classes\n",
    "    # math.ceil gets the next larger integer number\n",
    "    nclasses = math.ceil(len(data) / nvals)\n",
    "    \n",
    "    # split array into n different classes\n",
    "    data_split = np.array_split(data, nclasses)\n",
    "    \n",
    "    # get center of class and variogram values\n",
    "    ccenters = []\n",
    "    var_values = []\n",
    "    \n",
    "    # iterate over each class\n",
    "    for c in data_split:\n",
    "        # calculate class center ((min - max) / 2)\n",
    "        ccenter = (c.iloc[-1, class_col] + c.iloc[0, class_col]) / 2\n",
    "        # calculate variogram value\n",
    "        # Note: here, the difference is NOT SQUARED because we already work with\n",
    "        #     squared height differences\n",
    "        var_value = (c.iloc[:, var_col].sum() / (2 * len(c)))\n",
    "        # append the calculated values to the lists\n",
    "        ccenters.append(ccenter)\n",
    "        var_values.append(var_value)\n",
    "    \n",
    "    if plotting:\n",
    "        #Plot the empirical variogram\n",
    "        fig = plt.figure(figsize=(8,6))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.scatter(ccenters, var_values, marker=\"x\", c=\"black\", label=\"empirical variogram\")\n",
    "        # labels etc.\n",
    "        ax.set_xlabel(\"distance [m]\")\n",
    "        ax.set_ylabel(\"variogram $[m^2]$\")\n",
    "        ax.set_title(\"Empirical Variogram\")\n",
    "        ax.grid(True, alpha=0.4, zorder=0)\n",
    "        \n",
    "    return ccenters, var_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a1fa9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5742c745494d638b6be0da9fbd790b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ccenters, var_values = variogram(data=df_sorted, class_col=2, var_col=3, nvals=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2761c3da",
   "metadata": {},
   "source": [
    "Visually interpreting the empirical variogram, we can see that the variance among the values that are closer than 2000 m to each other is <1000 mÂ², which is much smaller than the absolute variance of the data set (<span>&asymp;</span>3600 mÂ²). This indicates a strong relationship between values, which are close to each other and means that we can estimate the ground height at a certain point very well, if we know the height at other point(s) in close vicinity. After 6000 m distance, however, the variance within the classes exceeds the absolute variance, sometimes even by far. <br>\n",
    "It might seem strange that some classes (=set of values with a certain distance to each other) show a much higher variance than the overall data set, but you should always keep in mind that our data is <b>not</b> normally distributed and that the choice of sampling points or spatial patterns within the data set can cause higher variations in ground height than the \"average\" variance.<br>\n",
    "However, with this empirical variogram, we can roughly say that the data set allows us to draw good conclusions on all points that are closer than 6000 m to our sampling points. <br>\n",
    "<br>\n",
    "But how can we apply this knowledge for points that are not at the exact distance of our class centers? For this, we will have to find an analytical function, describing our empirical variogram best:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d05bb2",
   "metadata": {},
   "source": [
    "### Defining Functions to Calculate Residuals Between Empirical and Theoretical Variogram Values\n",
    "For this, we will choose a certain set of theoretical variograms and define, how the height ($h$) depends on the distance ($d$). $\\sigma^2$ is the absolute variance of the ground height, and $\\lambda$, $\\alpha$ and $n$ are plotting parameters.<br>\n",
    "We will use four functions:\n",
    "- an exponential variogram <br>\n",
    "$\\large h(d)=\\sigma^2 \\cdot (1-e^{-d/{\\lambda}}) $\n",
    "- a Gaussian variogram <br>\n",
    "$\\large h(d)=\\sigma^2 \\cdot (1-e^{-(d/\\lambda)^2}) $\n",
    "- a spherical variogram<br>\n",
    "$\\large h(d)=\\sigma^2 \\cdot \\frac{h}{2\\lambda} \\cdot[3-(\\frac{d}{\\lambda})^2] $\n",
    "- a power function variogram<br>\n",
    "$\\large h(d)=n \\cdot d^{\\alpha} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44cab3a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exp_var(x, class_centers, y_true=None, fit=True):\n",
    "    \"\"\"\n",
    "    x : array [nugget, variance, correlation_length] of parameters, array\n",
    "    class_centers : class centers / x-values to calculate variogram at, array\n",
    "    y_true : observed variogram values (empirical variogram), array\n",
    "    \n",
    "    NOTES: this function takes class centers and observed variogram values\n",
    "    and returns an array of residuals between the theoretical function and\n",
    "    observed values\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the theoretical variogram value\n",
    "    exp = [(x[0] + (x[1] - x[0]) * (1 - np.exp(- i / x[2]))) for i in class_centers]\n",
    "    \n",
    "    if fit:\n",
    "        # if fit is True, return an array of residuals (i.e., the deviations\n",
    "        #     of the theoretical variogram values from the empirical variogram\n",
    "        #     values)\n",
    "        return np.array(np.array(exp) - np.array(y_true))\n",
    "    else:\n",
    "        # if fit is False, return only the theoretical variogram values at\n",
    "        #     the given x-values\n",
    "        return exp\n",
    "        \n",
    "\n",
    "def gauss_var(x, class_centers, y_true=None, fit=True):\n",
    "    \"\"\"\n",
    "    x : array [nugget, variance, correlation_length] of parameters, array\n",
    "    class_centers : class centers to calculate variogram at, array\n",
    "    y_true : observed variogram values (empirical variogram), array\n",
    "    \n",
    "    NOTES: this function takes class centers and observed variogram values\n",
    "    and returns an array of residuals between the theoretical function and\n",
    "    observed values\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the theoretical variogram value\n",
    "    gauss = [(x[0] + (x[1] - x[0]) * (1 - np.exp(-1 * ((i / x[2]) ** 2)))) for i in class_centers]\n",
    "    \n",
    "    if fit:\n",
    "        # if fit is True, return an array of residuals (i.e., the deviations\n",
    "        #     of the theoretical variogram values from the empirical variogram\n",
    "        #     values)\n",
    "        return np.array(np.array(gauss) - np.array(y_true))\n",
    "    else:\n",
    "        # if fit is False, return only the theoretical variogram values at\n",
    "        #     the given x-values\n",
    "        return gauss\n",
    "\n",
    "def spherical_var(x, class_centers, y_true=None, fit=True):\n",
    "    \"\"\"\n",
    "    x : array [nugget, variance, correlation_length] of parameters, array\n",
    "    class_centers : class centers to calculate variogram at, array\n",
    "    y_true : observed variogram values (empirical variogram), array\n",
    "    \n",
    "    NOTES: this function takes class centers and observed variogram values\n",
    "    and returns an array of residuals between the theoretical function and\n",
    "    observed values\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the theoretical variogram value\n",
    "    spherical = [(x[0] + (x[1] - x[0]) * 0.5 * i / x[2] * (3 - (i / x[2]) ** 2)) if i < x[2] else x[1] for i in class_centers]\n",
    "    \n",
    "    if fit:\n",
    "        # if fit is True, return an array of residuals (i.e., the deviations\n",
    "        #     of the theoretical variogram values from the empirical variogram\n",
    "        #     values)\n",
    "        return np.array(np.array(spherical) - np.array(y_true))\n",
    "    else:\n",
    "        # if fit is False, return only the theoretical variogram values at\n",
    "        #     the given x-values\n",
    "        return spherical\n",
    "\n",
    "def power_var(x, class_centers, y_true=None, fit=True):\n",
    "    \"\"\"\n",
    "    x : array [nugget, slope, power] of parameters, array\n",
    "    class_centers : class centers to calculate variogram at, array\n",
    "    y_true : observed variogram values (empirical variogram), array\n",
    "    \n",
    "    NOTES: this function takes class centers and observed variogram values\n",
    "    and returns an array of residuals between the theoretical function and\n",
    "    observed values\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the theoretical variogram value\n",
    "    power = [(x[0] + (x[1] - x[0]) * i ** x[2]) for i in class_centers]\n",
    "    \n",
    "    if fit:\n",
    "        # if fit is True, return an array of residuals (i.e., the deviations\n",
    "        #     of the theoretical variogram values from the empirical variogram\n",
    "        #     values)\n",
    "        return np.array(np.array(power) - np.array(y_true))\n",
    "    else:\n",
    "        # if fit is False, return only the theoretical variogram values at\n",
    "        #     the given x-values\n",
    "        return power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1056410a",
   "metadata": {},
   "source": [
    "### Fitting and Plotting Empirical and Fitted Theoretical Variograms\n",
    "Now, we will fit the functions for our theoretical variograms to our empirical variograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c9373ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_variograms(class_centers, variogram_values, manual_fitting=False,\n",
    "                    exp_ng=1000.,\n",
    "                    exp_vr=1000.,\n",
    "                    exp_cl=1000.,\n",
    "                    gau_ng=1000.,\n",
    "                    gau_vr=1000.,\n",
    "                    gau_cl=1000.,\n",
    "                    sph_ng=1000.,\n",
    "                    sph_vr=1000.,\n",
    "                    sph_cl=1000.,\n",
    "                    pwr_ng=0.,\n",
    "                    pwr_sp=1.,\n",
    "                    pwr_pw=1.):\n",
    "    \"\"\"\n",
    "    class_centers : class centers, array\n",
    "    variogram_values : variogram values, array\n",
    "    manual_fitting : whether to manually fit parameters in the theoretical variograms\n",
    "        or to fit with scipy.least_squares, bool\n",
    "    (exp / gau / sph / pwr)_ng : nugget effect for exponential / gaussian / spherical / power variogram; floats\n",
    "    (exp / gau / sph)_vr : variance for exponential / gaussian / spherical variogram; floats\n",
    "    (exp / gau / sph)_cl : corr. length for exponential / gaussian / spherical variogram; floats\n",
    "    pwr_sp : slope for power variogram; float\n",
    "    pwr_pw : power for power variogram; float\n",
    "    \"\"\"\n",
    "    \n",
    "    # x-values for theoretical variograms\n",
    "    xs = np.arange(0, max(class_centers), 100)\n",
    "    \n",
    "    \"\"\" exponential variogram \"\"\"\n",
    "    if manual_fitting:\n",
    "        # change these parameters for manual fitting\n",
    "        # [nugget, variance, correlation_length]\n",
    "        params_exp = [exp_ng, exp_vr, exp_cl]\n",
    "    else:\n",
    "        # scipy least squares optimization for exp\n",
    "        exp_x0 = np.array([0, 3000, 3000])\n",
    "        result_exp = least_squares(\n",
    "            exp_var,\n",
    "            exp_x0,\n",
    "            kwargs={\"class_centers\": class_centers, \"y_true\": variogram_values},\n",
    "            bounds=([0, 0, 0], [1000, 10000, 10000])\n",
    "        )\n",
    "        params_exp = result_exp.x\n",
    "        resids = result_exp.fun ** 2\n",
    "        ssres = resids.sum()\n",
    "        sstot = np.array([(i - np.array(var_values).mean()) ** 2 for i in var_values]).sum()\n",
    "        r2_exp = 1 - (ssres / sstot)\n",
    "    \n",
    "    # fitted exponential variogram    \n",
    "    exp_fit = [(params_exp[0] + (params_exp[1] - params_exp[0]) * (1 - np.exp(- i / params_exp[2]))) for i in xs]\n",
    "    \n",
    "    \"\"\" gaussian variogram \"\"\"\n",
    "    if manual_fitting:\n",
    "        # change these parameters for manual fitting\n",
    "        # [nugget, variance, correlation_length]\n",
    "        params_gauss = [gau_ng, gau_vr, gau_cl]\n",
    "    else:\n",
    "        # scipy least squares optimization for gauss\n",
    "        gauss_x0 = np.array([0, 3000, 3000])\n",
    "        result_gauss = least_squares(\n",
    "            gauss_var,\n",
    "            gauss_x0,\n",
    "            kwargs={\"class_centers\": class_centers, \"y_true\": variogram_values},\n",
    "            bounds=([0, 0, 0], [1000, 10000, 10000])\n",
    "        )\n",
    "        params_gauss = result_gauss.x\n",
    "        resids = result_gauss.fun ** 2\n",
    "        ssres = resids.sum()\n",
    "        sstot = np.array([(i - np.array(var_values).mean()) ** 2 for i in var_values]).sum()\n",
    "        r2_gauss = 1 - (ssres / sstot)\n",
    "    \n",
    "    # fitted gauss variogram    \n",
    "    gauss_fit = [(params_gauss[0] + (params_gauss[1] - params_gauss[0]) * (1 - np.exp(-1 * ((i / params_gauss[2]) ** 2)))) for i in xs]\n",
    "    \n",
    "    \"\"\" shperical variogram \"\"\"\n",
    "    if manual_fitting:\n",
    "        # change these parameters for manual fitting\n",
    "        # [nugget, variance, correlation_length]\n",
    "        params_spherical = [sph_ng, sph_vr, sph_cl]\n",
    "    else:\n",
    "        # scipy least squares optimization for spherical\n",
    "        spherical_x0 = np.array([0, 3000, 3000])\n",
    "        result_spherical = least_squares(\n",
    "            spherical_var,\n",
    "            spherical_x0,\n",
    "            kwargs={\"class_centers\": class_centers, \"y_true\": variogram_values},\n",
    "            bounds=([0, 0, 0], [1000, 10000, 10000])\n",
    "        )\n",
    "        params_spherical = result_spherical.x\n",
    "        resids = result_spherical.fun ** 2\n",
    "        ssres = resids.sum()\n",
    "        sstot = np.array([(i - np.array(var_values).mean()) ** 2 for i in var_values]).sum()\n",
    "        r2_spherical = 1 - (ssres / sstot)\n",
    "    \n",
    "    # fitted spherical variogram    \n",
    "    spherical_fit = [(params_spherical[0] + (params_spherical[1] - params_spherical[0]) * 0.5 * i / params_spherical[2] * \\\n",
    "                      (3 - (i / params_spherical[2]) ** 2)) if i < params_spherical[2] else params_spherical[1] for i in xs]\n",
    "    \n",
    "    \"\"\" power variogram \"\"\"\n",
    "    if manual_fitting:\n",
    "        # change these parameters for manual fitting\n",
    "        # [nugget, slope, power]\n",
    "        params_power = [pwr_ng, pwr_sp, pwr_pw]\n",
    "    else:\n",
    "        # scipy least squares optimization for power\n",
    "        power_x0 = np.array([0, 1, 1])\n",
    "        result_power = least_squares(\n",
    "            power_var,\n",
    "            power_x0,\n",
    "            kwargs={\"class_centers\": class_centers, \"y_true\": variogram_values},\n",
    "            bounds=([0, 0, 0], [1000, 10000, 10000])\n",
    "        )\n",
    "        params_power = result_power.x\n",
    "        resids = result_power.fun ** 2\n",
    "        ssres = resids.sum()\n",
    "        sstot = np.array([(i - np.array(var_values).mean()) ** 2 for i in var_values]).sum()\n",
    "        r2_power = 1 - (ssres / sstot)\n",
    "    \n",
    "    # fitted power variogram    \n",
    "    power_fit = [(params_power[0] + (params_power[1] - params_power[0]) * i ** params_power[2]) for i in xs]\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.scatter(class_centers, variogram_values, marker=\"x\", c=\"black\", label=\"empirical variogram\")\n",
    "    \n",
    "    ax.plot(xs, exp_fit, label=\"Exponential\", c=\"red\")\n",
    "    ax.plot(xs, gauss_fit, label=\"Gaussian\", c=\"deepskyblue\")\n",
    "    ax.plot(xs, spherical_fit, label=\"Spherical\", c=\"limegreen\")\n",
    "    ax.plot(xs, power_fit, label=\"Power\", c=\"purple\")\n",
    "    \n",
    "    # print out the fitted parameters\n",
    "    ax.text(1.05, 0.8, \"Exponential: \\n  nugget = %1.2f \\n  variance = %1.2f \\n  corr. length = %1.2f \\n  $R^2$ = %1.2f\" \\\n",
    "            %(params_exp[0], params_exp[1], params_exp[2], r2_exp), transform=ax.transAxes, fontsize=12)\n",
    "    \n",
    "    ax.text(1.05, 0.55, \"Gaussian: \\n  nugget = %1.2f \\n  variance = %1.2f \\n  corr. length = %1.2f \\n  $R^2$ = %1.2f\" \\\n",
    "            %(params_gauss[0], params_gauss[1], params_gauss[2], r2_gauss), transform=ax.transAxes, fontsize=12)\n",
    "    \n",
    "    ax.text(1.05, 0.3, \"Spherical: \\n  nugget = %1.2f \\n  variance = %1.2f \\n  corr. length = %1.2f  \\n  $R^2$ = %1.2f\" \\\n",
    "            %(params_spherical[0], params_spherical[1], params_spherical[2], r2_spherical), transform=ax.transAxes, fontsize=12)\n",
    "    \n",
    "    ax.text(1.05, 0.05, \"Power: \\n  nugget = %1.2f \\n  slope = %1.2f \\n  power = %1.2f \\n  $R^2$ = %1.2f\" \\\n",
    "            %(params_power[0], params_power[1], params_power[2], r2_power), transform=ax.transAxes, fontsize=12)\n",
    "    \n",
    "    ax.legend(loc=\"best\", fontsize=14)\n",
    "    ax.set_xlabel(\"distance [m]\", fontsize=14)\n",
    "    ax.set_ylabel(\"variogram $[m^2]$\", fontsize=14)\n",
    "    ax.set_title(\"Empirical and Fitted Theoretical Variograms\", fontsize=16)\n",
    "    ax.set_ylim(0, 8500)\n",
    "    ax.set_xlim(0, 20000)\n",
    "    ax.grid(True, alpha=0.4, zorder=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return result_exp.x, result_gauss.x, result_spherical.x, result_power.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b53ea",
   "metadata": {},
   "source": [
    "### Creating Empirical Variogram Values and Fit Theoretical Variograms\n",
    "\n",
    "- try different number of values in the classes\n",
    "- try different initial conditions for theoretical variograms\n",
    "- try different parameter bounds for theoretical variograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e4a8459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab267fa3fbe48fab6b49ab72fb5ccf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ccenters, var_values = variogram(data=df_sorted, class_col=2, var_col=3, nvals=100, plotting=False)\n",
    "p_exp, p_gauss, p_spherical, p_power = plot_variograms(class_centers=ccenters, variogram_values=var_values, manual_fitting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b4df02-e3a2-4638-a675-14ea87f4930d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Kriging\n",
    "\n",
    "We now define the Kriging system of equations:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "-\\gamma(\\mathbf{h}_{1, 1}) & \\dots & -\\gamma(\\mathbf{h}_{1, n}) & 1 \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "-\\gamma(\\mathbf{h}_{n, 1}) & \\dots & -\\gamma(\\mathbf{h}_{n, n}) & 1 \\\\\n",
    "1 & \\dots & 1 & 0\n",
    "\\end{pmatrix} \\cdot\n",
    "\\begin{pmatrix}\n",
    "w_1 \\\\\n",
    "\\vdots \\\\\n",
    "w_n \\\\\n",
    "\\Lambda\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "-\\gamma(\\mathbf{h}^*_{1}) \\\\\n",
    "\\vdots \\\\\n",
    "-\\gamma(\\mathbf{h}^*_{n}) \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e2489",
   "metadata": {},
   "source": [
    "### Define Functions to Get Theoretical Variogram Values for All Pairs of Points and to Make Point Estimates from Kriging Matrix\n",
    "\n",
    "We get the red part of the matrix from the first function:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\color{red}{-\\gamma(\\mathbf{h}_{1, 1})} & \\color{red}{\\dots} & \\color{red}{-\\gamma(\\mathbf{h}_{1, n})} & 1 \\\\\n",
    "\\color{red}{\\vdots} & \\color{red}{\\ddots} & \\color{red}{\\vdots} & \\vdots \\\\\n",
    "\\color{red}{-\\gamma(\\mathbf{h}_{n, 1})} & \\color{red}{\\dots} & \\color{red}{-\\gamma(\\mathbf{h}_{n, n})} & 1 \\\\\n",
    "1 & \\dots & 1 & 0\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bcb7564",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_variogram_vals(distances, method, params):\n",
    "    \"\"\"\n",
    "    distances : pandas DataFrame of shape (N_points x N_points) containing all distances between observation points, pd.DataFrame\n",
    "    method : variogram method to use, str - options are:\n",
    "        \"exponential\"\n",
    "        \"gauss\"\n",
    "        \"spherical\"\n",
    "        \"power\"\n",
    "    params : 1D list or array containing fitted variogram parameters, list or np.array\n",
    "    \"\"\"\n",
    "    \n",
    "    # make array to store calculated variogram values\n",
    "    obs_var_vals = []\n",
    "    \n",
    "    for i in range(len(distances)):\n",
    "        row = distances.iloc[i, :]\n",
    "        if method == \"exponential\":\n",
    "            var_results = exp_var(x=params, class_centers=row, fit=False)\n",
    "        elif method == \"gauss\":\n",
    "            var_results = gauss_var(x=params, class_centers=row, fit=False)\n",
    "        elif method == \"spherical\":\n",
    "            var_results = spherical_var(x=params, class_centers=row, fit=False)\n",
    "        elif method == \"power\":\n",
    "            var_results = power_var(x=params, class_centers=row, fit=False)\n",
    "        obs_var_vals.append(var_results)\n",
    "    \n",
    "    obs_var_vals = pd.DataFrame(obs_var_vals)\n",
    "            \n",
    "    return obs_var_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1693b3-e021-4e00-a84b-1c278cb0fe24",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Perform Kriging / Make an Estimate for a Given Point\n",
    "\n",
    "Starting from the available red parts of the matrix from the previous function:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\color{red}{-\\gamma(\\mathbf{h}_{1, 1})} & \\color{red}{\\dots} & \\color{red}{-\\gamma(\\mathbf{h}_{1, n})} & 1 \\\\\n",
    "\\color{red}{\\vdots} & \\color{red}{\\ddots} & \\color{red}{\\vdots} & \\vdots \\\\\n",
    "\\color{red}{-\\gamma(\\mathbf{h}_{n, 1})} & \\color{red}{\\dots} & \\color{red}{-\\gamma(\\mathbf{h}_{n, n})} & 1 \\\\\n",
    "1 & \\dots & 1 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In the following function, we now first create the orange part:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\color{red}{-\\gamma(\\mathbf{h}_{1, 1})} & \\color{red}{\\dots} & \\color{red}{-\\gamma(\\mathbf{h}_{1, n})} & \\color{orange}{1} \\\\\n",
    "\\color{red}{\\vdots} & \\color{red}{\\ddots} & \\color{red}{\\vdots} & \\color{orange}{\\vdots} \\\\\n",
    "\\color{red}{-\\gamma(\\mathbf{h}_{n, 1})} & \\color{red}{\\dots} & \\color{red}{-\\gamma(\\mathbf{h}_{n, n})} & \\color{orange}{1} \\\\\n",
    "\\color{orange}{1} & \\color{orange}{\\dots} & \\color{orange}{1} & \\color{orange}{0}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "And then, we calculate the variogram values (blue) for the point of interest (our estimation point):\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\color{turquoise}{-\\gamma(\\mathbf{h}^*_{1})} \\\\\n",
    "\\color{turquoise}{\\vdots} \\\\\n",
    "\\color{turquoise}{-\\gamma(\\mathbf{h}^*_{n})} \\\\\n",
    "\\color{turquoise}{1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Finally, we put everything together; we get the inverse of the kriging matrix (red $ -1 $) and multiply it with the new variogram values to get weights and the Lagrange multiplier (green):\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\color{red}{-\\gamma(\\mathbf{h}_{1, 1})} & \\color{red}{\\dots} & \\color{red}{-\\gamma(\\mathbf{h}_{1, n})} & \\color{orange}{1} \\\\\n",
    "\\color{red}{\\vdots} & \\color{red}{\\ddots} & \\color{red}{\\vdots} & \\color{orange}{\\vdots} \\\\\n",
    "\\color{red}{-\\gamma(\\mathbf{h}_{n, 1})} & \\color{red}{\\dots} & \\color{red}{-\\gamma(\\mathbf{h}_{n, n})} & \\color{orange}{1} \\\\\n",
    "\\color{orange}{1} & \\color{orange}{\\dots} & \\color{orange}{1} & \\color{orange}{0}\n",
    "\\end{pmatrix}^{\\color{red}{-1}} \\cdot\n",
    "\\begin{pmatrix}\n",
    "\\color{turquoise}{-\\gamma(\\mathbf{h}^*_{1})} \\\\\n",
    "\\color{turquoise}{\\vdots} \\\\\n",
    "\\color{turquoise}{-\\gamma(\\mathbf{h}^*_{n})} \\\\\n",
    "\\color{turquoise}{1}\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "\\color{limegreen}{w_1} \\\\\n",
    "\\color{limegreen}{\\vdots} \\\\\n",
    "\\color{limegreen}{w_n} \\\\\n",
    "\\color{limegreen}{\\Lambda}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "616a0d82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_estimate(th_var, obs, poi, method, params):\n",
    "    \"\"\"\n",
    "    th_var : calculated theoretical variogram values for all point pairs, pd.DataFrame\n",
    "    obs : locations and measurements of the observation points containing three columns [Easting / Rechtswert, Northing / Hochwert, measurement], pd.DataFrame\n",
    "    poi : point of interest in the form [Easting / Rechtswert, Northing / Hochwert] to calculate weights for, list\n",
    "    method : method : variogram method to use, str - options are:\n",
    "        \"exponential\"\n",
    "        \"gauss\"\n",
    "        \"spherical\"\n",
    "        \"power\"\n",
    "    params : 1D list or array containing fitted variogram parameters, list or np.array\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate kriging matrix\n",
    "    # copy original DataFrame and invert sign\n",
    "    df = - th_var.copy()\n",
    "    \n",
    "    # append lagrange multiplier column\n",
    "    lagrange = [1 for i in range(df.shape[0])]\n",
    "    df[\"lagrange\"] = lagrange\n",
    "    \n",
    "    # append lagrange multiplier row\n",
    "    lagrange.append(0)\n",
    "    df.loc[len(df)] = lagrange\n",
    "    \n",
    "    # calculate distances between poi and all other points\n",
    "    distances_poi = []\n",
    "    for i in range(obs.shape[0]):\n",
    "        dist = ((poi[0] - obs.iloc[i, 0]) ** 2 + (poi[1] - obs.iloc[i, 1]) ** 2) ** 0.5\n",
    "        distances_poi.append(dist)\n",
    "    \n",
    "    # calculate variogram values for poi w.r.t. all other points\n",
    "    if method == \"exponential\":\n",
    "        poi_var_vals = - np.array(exp_var(x=params, class_centers=distances_poi, fit=False))\n",
    "        poi_var_vals = np.append(poi_var_vals, 1)\n",
    "    elif method == \"gauss\":\n",
    "        poi_var_vals = - np.array(gauss_var(x=params, class_centers=distances_poi, fit=False))\n",
    "        poi_var_vals = np.append(poi_var_vals, 1)\n",
    "    elif method == \"spherical\":\n",
    "        poi_var_vals = - np.array(spherical_var(x=params, class_centers=distances_poi, fit=False))\n",
    "        poi_var_vals = np.append(poi_var_vals, 1)\n",
    "    elif method == \"power\":\n",
    "        poi_var_vals = - np.array(power_var(x=params, class_centers=distances_poi, fit=False))\n",
    "        poi_var_vals = np.append(poi_var_vals, 1)\n",
    "    \n",
    "    # calculate weights\n",
    "    # calculate weights from linear system of equations (w = poi_var_vals * df^-1)\n",
    "    kriging_matrix_inv = np.linalg.inv(df.values)\n",
    "    w = np.matmul(kriging_matrix_inv, poi_var_vals)\n",
    "    \n",
    "    # calculate estimated value\n",
    "    estimate = np.dot(w[:-1].T, obs.iloc[:, -1])\n",
    "    \n",
    "    return np.array([poi[0], poi[1], estimate])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34383f6f",
   "metadata": {},
   "source": [
    "### Make Point Estimates at Defined Grid Points in the Area\n",
    "We start by calculating the theoretical variogram values (using the fitted parameters) for all point pairs, i.e., distances. This is similar to the creation of the empirical variogram but we do not put the values in distinct classes. Having the calculated variogram values, we utilize the `make_estimate` function. This function takes the calculated variogram values, the actual observations and a point of interest and computes (1) the **kriging matrix** and the **kriging weights**. The weights are calculated using the **Lagrange multipliers**.\n",
    "We create a *grid* of test-points (points of interest) where we want to estimate the ground height from kriging weights and surrounding points. for every point in the grid we perform the method described above and get a grid of kriging results (or estimates). THese results can then be visualized.\n",
    "**NOTE**: The number of points to sample in each spatial direction quickly blows up the calculations ($ N_{points, total} = N_x \\cdot N_y $)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e629564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get observations (locations and measurements)\n",
    "obs = df.copy().iloc[:, 2:]\n",
    "\n",
    "# calculate variogram values from fitted theoretical variogram for all points\n",
    "df_k1 = get_variogram_vals(distances=pd.DataFrame(distances), method=\"exponential\", params=p_exp)\n",
    "\n",
    "# set number of points to calculate kriging weights for in x- and y-direction\n",
    "# this results in a total of (num x num) test points to calculate weights for\n",
    "# Note: this effectively defines the \"resolution\" of our results, i.e., with an\n",
    "#     increasing num, more points are estimated\n",
    "num = 20\n",
    "\n",
    "# make points for calculations\n",
    "# linspace givesa list / array of linearly spaced points in an interval\n",
    "xs = np.linspace(start=df.iloc[:, 2].min(), stop=df.iloc[:, 2].max(), num=num, endpoint=True)\n",
    "ys = np.linspace(start=df.iloc[:, 3].min(), stop=df.iloc[:, 3].max(), num=num, endpoint=True)\n",
    "\n",
    "# make array to store estimates\n",
    "estimates = []\n",
    "\n",
    "# get estimates (iterate over all x- and y-coordinates)\n",
    "for x in xs:\n",
    "    for y in ys:\n",
    "        estimate = make_estimate(th_var=df_k1, obs=obs, poi=[x, y], method=\"exponential\", params=p_exp)\n",
    "        estimates.append(estimate)\n",
    "        \n",
    "estimates_df = pd.DataFrame(data=estimates, columns=[\"x\", \"y\", \"estimate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dcc21c",
   "metadata": {},
   "source": [
    "### Prepare Results for Plotting\n",
    "The results can be plotted as (1) triangulated surface or (2) contour lines. Specify `kind` in `plot_results` to control the plotting method (`\"surface\"` or `\"contours\"`). The number of contour lines can be changed via `n_contours`.\n",
    "Contour plotting has the additional benefit that a high resolution can be obtained in the plot while having only a small number of sampling points (interpolation increases resolution when the number of contours is high). The triangulation method resolution relies completely on the sampling density (higher points density gives better resolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "108764de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncomment widget to get interactive plot\n",
    "# uncomment inline to get static plot\n",
    "# always have one of the options commented!!!\n",
    "\n",
    "%matplotlib widget\n",
    "# %matplotlib inline\n",
    "\n",
    "\n",
    "def plot_results(xs, ys, data, observations, kind, n_contours=20, rot_x=50, rot_z=140):\n",
    "    \"\"\"\n",
    "    xs : 1D array of x-values for data, array\n",
    "    ys : 1D array of y-values for data, array\n",
    "    data : 1D array of data (results), array\n",
    "    observations : pd.DataFrame containing points and measurements of observations, pd.DataFrame\n",
    "    kind : plotting method; either \"surface\" or \"contours\", str\n",
    "    n_contours : number of contour lines to draw (is ignored if kind == \"surface\"), int\n",
    "    rot_x : x-axis rotation of view, 0 <= float <= 360\n",
    "    rot_z : z-axis rotation of view, 0 <= float <= 360\n",
    "    \"\"\"\n",
    "    \n",
    "    # set up figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw={\"projection\": \"3d\"})\n",
    "    \n",
    "    # make meshgrid from x- and y-values\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    \n",
    "    if kind == \"surface\":\n",
    "        X, Y = X.flatten(), Y.flatten()\n",
    "        # triangulate\n",
    "        tri = matplotlib.tri.Triangulation(X, Y)\n",
    "    \n",
    "        # plot triangulated surfaces\n",
    "        im = ax.plot_trisurf(X, Y, data, triangles=tri.triangles, cmap=\"terrain\", alpha=0.8)\n",
    "    \n",
    "    elif kind == \"contours\":\n",
    "        data = np.reshape(data, (-1, len(xs)))\n",
    "        im = ax.contour(X, Y, data, cmap=\"terrain\", levels=50)\n",
    "    \n",
    "    # scatter measurements\n",
    "    ax.scatter(observations.iloc[:, 0], observations.iloc[:, 1], observations.iloc[:, 2], c=\"red\", s=50)\n",
    "    \n",
    "    cbar = plt.colorbar(im, shrink=0.3, ax=ax)\n",
    "    cbar.set_label(\"height [m a.s.l.]\")\n",
    "    ax.set_xlabel(\"Easting / Rechtswert [m]\")\n",
    "    ax.set_ylabel(\"Northing / Hochwert [m]\")\n",
    "    ax.set_zlabel(\"Height [m a.s.l.]\")\n",
    "    ax.set_title(\"Kriging Results\")\n",
    "    \n",
    "    # set view direction\n",
    "    ax.view_init(rot_x, rot_z)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return ax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c11efe3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0113059e56ba4e99ae62b4d08eb45c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Axes3DSubplot:title={'center':'Kriging Results'}, xlabel='Easting / Rechtswert [m]', ylabel='Northing / Hochwert [m]'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_results(xs=xs, ys=ys, data=np.array(estimates_df.iloc[:, -1]), observations=obs, kind=\"surface\", n_contours=100, rot_x=30, rot_z=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b58f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
